{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 21CS30064: Ishaan Sinha\n",
    "# IHMCDC\n",
    "# Interest Match using Complete Linkage Divisive (Top-Down) Clustering Technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Define a function to calculate cosine similarity between two vectors\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    # Calculate the dot product of the two vectors\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    # Calculate the norms of the vectors\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    \n",
    "    # Check if either norm is zero to avoid division by zero\n",
    "    if norm_vec1 == 0 or norm_vec2 == 0:\n",
    "        return 0\n",
    "    \n",
    "    # Calculate the cosine similarity\n",
    "    cosine_sim = dot_product / (norm_vec1 * norm_vec2)\n",
    "    return cosine_sim\n",
    "\n",
    "# Define a function to initialize cluster centroids randomly\n",
    "def initialize_clusters(data, k):\n",
    "    # Randomly choose indices for initial centroids\n",
    "    centroids_indices = np.random.choice(data.shape[0], size=k, replace=False)\n",
    "    # Get the data points corresponding to the chosen indices\n",
    "    centroids = data[centroids_indices]\n",
    "    return centroids\n",
    "\n",
    "# Define a function to assign data points to clusters based on similarity to centroids\n",
    "def assign_to_clusters(data, centroids):\n",
    "    # Initialize a matrix to store similarities between data points and centroids\n",
    "    similarity_matrix = np.zeros((data.shape[0], centroids.shape[0]))\n",
    "    # Calculate cosine similarity between each data point and each centroid\n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(centroids.shape[0]):\n",
    "            similarity_matrix[i, j] = cosine_similarity(data[i], centroids[j])\n",
    "    # Assign each data point to the cluster with the highest similarity\n",
    "    labels = np.argmax(similarity_matrix, axis=1)\n",
    "    return labels\n",
    "\n",
    "# Define a function to update centroids based on assigned data points\n",
    "def update_centroids(data, labels, k):\n",
    "    # Initialize an array to store updated centroids\n",
    "    centroids = np.zeros((k, data.shape[1]))\n",
    "    # Iterate over each cluster\n",
    "    for cluster_label in range(k):\n",
    "        # Extract data points belonging to the current cluster\n",
    "        cluster_data = data[labels == cluster_label]\n",
    "        # Calculate the mean of the data points to obtain the new centroid\n",
    "        if len(cluster_data) > 0:\n",
    "            centroids[cluster_label] = np.mean(cluster_data, axis=0)\n",
    "        else:\n",
    "            centroids[cluster_label] = np.zeros(data.shape[1])\n",
    "    return centroids\n",
    "\n",
    "# Define a function to perform K-means clustering\n",
    "def k_means_clustering(data, k, iterations):\n",
    "    # Initialize centroids\n",
    "    centroids = initialize_clusters(data, k)\n",
    "    # Iterate for a fixed number of iterations\n",
    "    for _ in range(iterations):\n",
    "        # Assign data points to clusters\n",
    "        labels = assign_to_clusters(data, centroids)\n",
    "        # Update centroids based on assigned data points\n",
    "        centroids = update_centroids(data, labels, k)\n",
    "    return labels, centroids\n",
    "\n",
    "# Define a function to calculate the silhouette coefficient for clustering quality evaluation\n",
    "def silhouette_coefficient(data, labels, centroids):\n",
    "    # Get the number of samples\n",
    "    num_samples = len(data)\n",
    "    # Initialize an array to store silhouette scores for each sample\n",
    "    scores = np.zeros(num_samples)\n",
    "\n",
    "    # Calculate silhouette score for each sample\n",
    "    for i in range(num_samples):\n",
    "        # Calculate distance within the same cluster (a)\n",
    "        a = np.mean(np.linalg.norm(data[i] - data[labels == labels[i]], axis=1))\n",
    "        # Calculate distance to the nearest cluster (b)\n",
    "        b = np.min(np.mean(np.linalg.norm(data[i] - centroids, axis=1)))\n",
    "        # Calculate silhouette coefficient for the current sample\n",
    "        scores[i] = (b - a) / max(a, b)\n",
    "\n",
    "    # Calculate the mean silhouette coefficient across all samples\n",
    "    return np.mean(scores)\n",
    "\n",
    "# Define a function to compute Jaccard similarity between two sets of cluster labels\n",
    "def jaccard_similarity(label1, label2):\n",
    "    # Initialize lists to store indices of samples belonging to each cluster in both sets\n",
    "    list1 = []\n",
    "    list2 = []\n",
    "    # Calculate the number of clusters in each set\n",
    "    len1 = len(np.unique(label1))\n",
    "    len2 = len(np.unique(label2))\n",
    "    \n",
    "    # Iterate over each cluster in the first set\n",
    "    for i in range(len1):\n",
    "        temp = []\n",
    "        # Find indices of samples belonging to the current cluster in the first set\n",
    "        for j in range(len(label1)):\n",
    "            if int(label1[j]) == i:\n",
    "                temp.append(j)\n",
    "        list1.append(temp)\n",
    "    \n",
    "    # Iterate over each cluster in the second set\n",
    "    for i in range(len2):\n",
    "        temp = []\n",
    "        # Find indices of samples belonging to the current cluster in the second set\n",
    "        for j in range(len(label2)):\n",
    "            if int(label2[j]) == i:\n",
    "                temp.append(j)\n",
    "        list2.append(temp)\n",
    "\n",
    "    # Initialize a list to store Jaccard similarity scores for each cluster\n",
    "    jaccard_scores = []\n",
    "\n",
    "    # Iterate over each cluster in the first set\n",
    "    for i in range(len(list1)):\n",
    "        # Initialize the maximum Jaccard similarity score for the current cluster\n",
    "        jaccard_max = -1\n",
    "        # Iterate over each cluster in the second set\n",
    "        for j in range(len(list2)):\n",
    "            # Calculate the intersection and union of the two clusters\n",
    "            intersection = len(set(list1[i]).intersection(list2[j]))\n",
    "            union = len(set(list1[i]).union(list2[j]))\n",
    "            # Calculate Jaccard similarity for the current pair of clusters\n",
    "            jaccard = intersection / union if union != 0 else 0\n",
    "            # Update the maximum Jaccard similarity score for the current cluster\n",
    "            jaccard_max = max(jaccard_max, jaccard)\n",
    "        # Append the maximum Jaccard similarity score to the list\n",
    "        jaccard_scores.append(jaccard_max)\n",
    "\n",
    "    # Print the Jaccard similarity scores for each cluster in the first set\n",
    "    for i in range(len(jaccard_scores)):\n",
    "        print(\"Jaccard Similarity for \", i+1, \"th cluster is \", jaccard_scores[i])\n",
    "\n",
    "# Define a class for complete linkage divisive clustering\n",
    "class CompleteLinkageDivisiveClustering:\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "        self.labels = None\n",
    "    \n",
    "    def fit(self, data):\n",
    "        # Get the number of samples\n",
    "        num_samples = len(data)\n",
    "        # Initialize cluster labels\n",
    "        self.labels = np.zeros(num_samples)\n",
    "        \n",
    "        # Continue splitting clusters until the desired number of clusters is reached\n",
    "        while len(np.unique(self.labels)) < self.k:\n",
    "            # Initialize a matrix to store pairwise cosine similarity between samples\n",
    "            similarity_matrix  = np.zeros((data.shape[0], data.shape[0]))\n",
    "            # Calculate pairwise cosine similarity between all samples\n",
    "            for i in range(data.shape[0]):\n",
    "                for j in range(data.shape[0]):\n",
    "                    similarity_matrix[i, j] = cosine_similarity(data[i], data[j])\n",
    "            # Get the current number of clusters\n",
    "            num_clusters = len(np.unique(self.labels))\n",
    "            # Initialize variables to track maximum similarity and cluster indices\n",
    "            maxxx = -1\n",
    "            indd1 = -1\n",
    "            indd2 = -1\n",
    "            cluster = -1\n",
    "            # Iterate over each existing cluster\n",
    "            for i in range(num_clusters):\n",
    "                # Get indices of samples belonging to the current cluster\n",
    "                cluster_indices = np.where(self.labels == i)[0]\n",
    "                ind1 = -1\n",
    "                ind2 = -1\n",
    "                max_distance = -1\n",
    "                # Iterate over each pair of samples within the current cluster\n",
    "                for j in range(len(cluster_indices)):\n",
    "                    for k in range(j+1, len(cluster_indices)):\n",
    "                        # Find the pair with the maximum similarity\n",
    "                        if similarity_matrix[cluster_indices[j], cluster_indices[k]] > max_distance:\n",
    "                            max_distance = similarity_matrix[cluster_indices[j], cluster_indices[k]]\n",
    "                            ind1 = cluster_indices[j]\n",
    "                            ind2 = cluster_indices[k]\n",
    "                # Update variables if the maximum similarity exceeds the current maximum\n",
    "                if(max_distance > maxxx):\n",
    "                    maxxx = max_distance\n",
    "                    indd1 = ind1\n",
    "                    indd2 = ind2\n",
    "                    cluster = i\n",
    "\n",
    "            # Get the label for the largest cluster after splitting\n",
    "            largest = np.max(self.labels)+1\n",
    "            # Get indices of samples belonging to the cluster chosen for splitting\n",
    "            cluster_indices_temp = np.where(self.labels == cluster)[0]\n",
    "            # Update cluster labels based on similarity between samples and split point\n",
    "            for i in range(len(cluster_indices_temp)):\n",
    "                if similarity_matrix[indd1, cluster_indices_temp[i]] > similarity_matrix[indd2, cluster_indices_temp[i]]:\n",
    "                    self.labels[cluster_indices_temp[i]] = largest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For k = 3, Silhouette Coefficient: -0.1466375193058558\n",
      "For k = 4, Silhouette Coefficient: -0.09645624017887024\n",
      "For k = 5, Silhouette Coefficient: -0.061490565007007995\n",
      "For k = 6, Silhouette Coefficient: -0.024205641207828095\n",
      "\n",
      "Optimal number of clusters (k): 6 with Silhouette Coefficient: -0.024205641207828095\n",
      "Files kmeans.txt and divisive.txt generated successfully.\n",
      "Jaccard Similarity for  1 th cluster is  0.2346002621231979\n",
      "Jaccard Similarity for  2 th cluster is  0.175\n",
      "Jaccard Similarity for  3 th cluster is  0.1507537688442211\n",
      "Jaccard Similarity for  4 th cluster is  0.16133333333333333\n",
      "Jaccard Similarity for  5 th cluster is  0.2248995983935743\n",
      "Jaccard Similarity for  6 th cluster is  0.16339869281045752\n",
      "--- 410.71196603775024 seconds runtime---\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Load the dataset\n",
    "    data = pd.read_csv('interests.csv')\n",
    "\n",
    "    # Drop the class labels if they exist\n",
    "    if 'class' in data.columns:\n",
    "        data = data.drop('class', axis=1)\n",
    "    \n",
    "    # Convert the data to a numpy array\n",
    "    data = data.values\n",
    "    \n",
    "    # In the dataset, wherever any row contains a blank cell, replace it with 0\n",
    "    data = np.nan_to_num(data)\n",
    "\n",
    "    # Define the range of k values to test\n",
    "    k_values = [3, 4, 5, 6]\n",
    "    iterations = 20\n",
    "    \n",
    "    # Track the highest Silhouette Coefficient and its corresponding k value\n",
    "    max_silhouette_score = -1\n",
    "    optimal_k = None\n",
    "    \n",
    "    # Iterate over each k value\n",
    "    for k in k_values:\n",
    "        # Perform K-means clustering\n",
    "        labels, centroids = k_means_clustering(data, k, iterations)\n",
    "  \n",
    "        # Calculate the Silhouette Coefficient\n",
    "        silhouette_score = silhouette_coefficient(data, labels, centroids)\n",
    "        \n",
    "        # Print Silhouette Coefficient for each k value\n",
    "        print(f\"For k = {k}, Silhouette Coefficient: {silhouette_score}\")\n",
    "        \n",
    "        # Update optimal k if the current Silhouette Coefficient is higher\n",
    "        if silhouette_score > max_silhouette_score:\n",
    "            max_silhouette_score = silhouette_score\n",
    "            optimal_k = k\n",
    "\n",
    "    # Perform K-means clustering with the optimal k value\n",
    "    Labels, Centroids = k_means_clustering(data, optimal_k, iterations)\n",
    "    \n",
    "    print(f\"\\nOptimal number of clusters (k): {optimal_k} with Silhouette Coefficient: {max_silhouette_score}\")\n",
    "\n",
    "    # Perform divisive clustering\n",
    "    divisive_clustering = CompleteLinkageDivisiveClustering(optimal_k)\n",
    "    divisive_clustering.fit(data)\n",
    "    \n",
    "    divisive_labels = divisive_clustering.labels\n",
    "    \n",
    "    # Generate kmeans.txt\n",
    "    with open('kmeans.txt', 'w') as f:\n",
    "        for i in range(optimal_k):\n",
    "            cluster_indices = np.where(Labels == i)[0]\n",
    "            cluster_indices = sorted(cluster_indices)\n",
    "            f.write(','.join(map(str, cluster_indices)) + '\\n')\n",
    "    \n",
    "    # Generate divisive.txt\n",
    "    with open('divisive.txt', 'w') as f:\n",
    "        for i in range(optimal_k):\n",
    "            cluster_indices = np.where(divisive_labels == i)[0]\n",
    "            cluster_indices = sorted(cluster_indices)\n",
    "            f.write(','.join(map(str, cluster_indices)) + '\\n')\n",
    "\n",
    "    print(\"Files kmeans.txt and divisive.txt generated successfully.\")\n",
    "\n",
    "    # Compute Jaccard similarity between K-means and divisive clustering results\n",
    "    jaccard_similarity(Labels, divisive_labels)\n",
    "\n",
    "print(\"--- %s seconds runtime---\" % (time.time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
